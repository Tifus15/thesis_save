\chapter{Conclusions}
Physics-informed neural networks showed great capability in learning hamiltonian systems directly from data, though the graph structure of GNNs presented a really interesting alternative to classic multilayer perceptrons. The main challenges for those models remain being computationally demanding and requiring huge, high-quality datasets. Most of the experiments we showed in this thesis would never have been possible with a normal-end PC, hence underlining how access to high-performance computing resources played a an important role, for instance, using the Lichtenberg Cluster and high-end Laptop.\\
In this study, we successfully generated datasets from the hamiltonian equations and subsequently demonstrated that most of state of the art models  can learn from the time-series data. This provides further evidence of the  data-driven models can encapsulate complicated physical systems.\\
Through the experiments we demonstrated the performance of physics-informed models and  tested our own models on our datasets. Although the results weren't 100 \% accurate enough due to hardware issues we managed to obtain promising outcomes.  

In addition, we envision future studies on the Kolmogorov-Arnold Networks, since this is a newly proposed architecture that blends graph structures with spline functions that have a learned shape as a parameter to model natural data. Preliminary studies suggest KAN can learn physics-based systems and partial differential equations, which would represent a potentially strong alternative to traditional MLPs and GNNs\cite{kan}. In essence, KAN may prove to be that key leap forward that the domain of physical machine learning is pushing for.
   