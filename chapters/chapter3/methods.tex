\chapter{Methods}
After previous chapters we should achieve basic understanding of neural network models and physics. With this knowledge we will introduce the methods which are capable to learn from dataset which are created trough dynamics of the systems. 
\section{New paradigm - NeuralODE}
Neural ODE is very interesting and important concept for solving the Ordinary Differential Equations and training the neural networks with trajectory data.\\
The default situation is described as:
\begin{equation}
	\dot{\mathbf{x}}=\mathbf{f}(\mathbf{x},t|\Theta)
\end{equation}
Where $\mathbf{f}(\mathbf{x},t|\Theta) $ represents the multilayer perception which creates vector field.
This method has it history from residual networks where their feed-forward network looks like euler step
\begin{equation}
	\mathbf{h}^{(i+1)} = \mathbf{h}^{(i)} + \mathbf{f}(\mathbf{h}^{(i)}|\Theta)
\end{equation}

In the paper \cite{neuralODE} it is introduced a adjoint method  which solves problem with vanishing gradients.\\
This is achieved trough continuous backpropagation which is stated in the paper.
We can compare it 
\begin{center}
	\begin{tabular}{ c c|c }
		 & residual network & adjoint method \\ 
		 \hline
		$a_t$ or $a(t)$: & $\frac{\partial L}{\partial z}$ & $\frac{\partial L}{\partial z(t)}$ \\  
		forward-pass: & $z_{i+1} = z_t + hf(z_t)$ & $z(t+1) = z(t)+\int_t^{t+1}f(t)dt$\\
		backward-pass: &$a_t = a_{t+1}+ha_{t+1}\frac{\partial f(z_t)}{\partial z_t}$ & $a(t) = a(t+1)+\int_{t+1}^ta(t)\frac{\partial f(z(t))}{\partial z(z)}dt$\\
		gradients:  & $\frac{\partial L}{\partial \theta}=ha_{t+h}\frac{\partial f(z(t),\Theta)}{\partial \Theta}$ & $\frac{\partial L}{\partial \theta}=\int_t^{t+1}a(t)\frac{f(z(t),\Theta)}{\partial \Theta}$   
	\end{tabular}
\end{center}


\section{Recurrent time steppers}
In chapter\ref{}  we introduced recurrent models which are RNN and GRU.
Those models are very good at learning the time series data. To make it more physics informed we applied an euler method which it makes to time stepper models:
\begin{eqnarray}
	[\mathbf{v}_i, \mathbf{h}_{i+1}] = \text{RNN}(\mathbf{x}_i,\mathbf{h}_i) \text{  or  }  \text{GRU}(\mathbf{x}_i,\mathbf{h}_i)\\
	\mathbf{x}_{i+1} = \mathbf{x}_i + \mathbf{v}_i \cdot dt
\end{eqnarray}
We describe it this procedure as a rollout. 
This model has one big static parameter dependency which is fixed $dt$. We suggest to not to change this parameter after training. In Chapter experimentation we will show the performance of RNN and GRU time stepper models.

\section{Hamiltonian Neural Network}
This model comes from the paper \cite{hnn} it is one of the physics informed models because it uses differentiation technique and applies the Hamiltonian equations. In this case the $\mathbf{x} = [\mathbf{q},\mathbf{p}]$ are canonical or natural coordinates(Cartesian space).
The forward pass looks like:
\begin{eqnarray}
	H_{\Theta} &=& \mathbf{f}(\mathbf{x}|\boldsymbol{\Theta})\\
	\frac{d\mathbf{x}}{dt}(\mathbf{x}) &=& \mathbf{J}\left[\frac{\partial H_{\Theta}}{\partial\mathbf{p}}(\mathbf{x}),-\frac{\partial H_{\Theta}}{\partial\mathbf{q}}(\mathbf{x})\right]^T
\end{eqnarray}
To make trajectory we use ODE solver
\begin{equation}
	\mathbf{x} = \text{odeint}\left(\frac{d\mathbf{x}}{dt},\mathbf{x}_0,t\right)
\end{equation} 
The odeint operator in this equation is a solver for ODEs like euler or RK4 method. In the experiments due the limitation of framework \texttt{pytorch} it is impossible to us ode solver based on adjoint method. \\
\section{Graph Neural Network}
Graph neural Networks are very new neural architecture which is available today. It is used broadly in social sciences and in chemistry like generating new chemical compounds.\\
The main part of GNN is a graph $\mathbf{G}$. It is defined as a collection of vertices and edges $\mathbf{G}=\{\mathbf{V},\mathbf{E}\}$ where $\mathbf{V} = \{\mathbf{v}_1,...,\mathbf{v}_i,...,\mathbf{v}_n\}$ has $n$ vertices
and $\mathbf{E} = \{\mathbf{e}_1,...,\mathbf{e}_i,...,\mathbf{e}_m\}$
has $m$ edges.\\
The edge is connection between to nodes it can be directed(only one direction between the nodes) or undirected(both direction between the nodes). This is very important because the graphs has many properties which we can use building it as Graph Neural Model.\\
This connectivity can be represented with adjacency matrix $\mathbf{A}$.
Adjacency matrix is denoted as $\mathbf{A}\in \{0,1\}^{n x n}$ 
\begin{equation}
\mathbf{A}_{ij} =	\left\{\begin{matrix}
		1 & if (\mathbf{v}_i , \mathbf{v}_j)\in\mathcal{E}\\
		0 & otherwise
	\end{matrix}\right.
\end{equation}

Most important property of Graphs is creating the normailzed Laplacian matrix. This is foundation of the idea about graph neural networks and it is defined as:
\begin{equation}
	\mathbf{L}= \mathbf{I} - \mathbf{D}^{\frac{1}{2}}\mathbf{A}\mathbf{D}^{\frac{1}{2}} 
\end{equation} where $\mathbf{D}$ is degree matrix. Degree of a node is the number of nodes that are adjecent to the node in question. This matrix has only diagonal elements.\\
We recognize 3 main training types of GNNs and that are graph- , edge and node - oriented learning.\\
In our work we will use node oriented type. 

\subsection{Convolutional Graph Neural Network}
First idea about graph neural network was Spectral Graph Network (SGN) \cite{} in signal proccesing tasks which is based on spectrality of the modified  Laplacian in form \begin{equation}
	\mathbf{L}= \mathbf{I} - \mathbf{D}^{\frac{1}{2}}\mathbf{W}\mathbf{D}^{\frac{1}{2}} 
\end{equation}where we exchange adjecency matrix with learnable weight matrix.
Because of cost of computation and hard backpropagation, the ChebNet was introduced. 
It was dependent of fourier transformation and had high computational comlpexity.
First Kipf and Welling used both ideas and simplified Laplacian to create broadly used architecture called Convolutional Graph Network.\\Because of instalibilty of Laplacian Kipf and Welling \cite{} renormalization trick which stabilize the network:
\begin{eqnarray}
	\tilde{\mathbf{A}} = \mathbf{A} +\mathbf{I}\\
	\tilde{\mathbf{A}} = \sum_i \tilde{\mathbf{A}}_{ii}\\
	\mathbf{L}\rightarrow\tilde{\mathbf{D}}^{\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{\frac{1}{2}}
\end{eqnarray} 
This renormalised Laplacian is a base of Convolutional Graph Neural Network tgat we know today.
Forward pass for convolutional graph Network:
\begin{equation}
	\mathbf{h}^{(i+1)}=\sigma(\tilde{\mathbf{D}}^{\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{\frac{1}{2}}\mathbf{W}\mathbf{h}^{(i)})
\end{equation}
This mathematical expression is equivalent to message passing and aggregation from and between the neighbouring nodes. In this case with $\sum operator$.\\ 
With knowledge that the neural network doesn't depend on graph structure but on massage passing and aggregation btween the nodes, we can build more designs of graph neural networks. We should not forget, that those message passing and agrregation can be paralized and calculated and GPUs improving performance. In our use cases we will use Framework \texttt{DGL}.


\subsection{Gated Attention Network}
In paper \cite{} is intoduced new type of architecture which incomporates weighting factors. This weightning factors $\alpha$ shows importance of the node j to the node in question.
The forward pass of one layer is defined as:
\begin{eqnarray}
\mathbf{z}_i^{(l)}&=&\mathbf{W}_f^{(l)}\mathbf{h}_i^{(l)} \\
e_{ij}^{(l)}&=&\text{LeakyReLU}(\mathbf{W_a}^{(l)^T}(\mathbf{z}_i^{(l)}||\mathbf{z}_j^{(l)}))\\
\alpha_{ij}^{(l)}&=&\frac{\exp(e_{ij}^{(l)})}{\sum_{k\in \mathcal{N}(i)}^{}\exp(e_{ik}^{(l)})}\\
\mathbf{h}_i^{(l+1)}&=&\sigma\left(\sum_{j\in \mathcal{N}(i)} {\alpha^{(l)}_{ij} \mathbf{z}^{(l)}_j }\right)
\end{eqnarray}

This architecture is computationally effcient and storage efficient. It is most important fixed which means it doesn't depend on number of nodes in data.
For better learning the datasets it is suggested to use dropout at attention.












\section{Graph Neural Hamiltonian Network}
This architecture is inspired on implementation of HNN and paper \cite{} which does simmilar models to it. This model is physics informed model. It is created exactly as HNN but the base model in this case isn't MLP but the GNN.

\begin{eqnarray}
	H_{\Theta} &=& \mathbf{f}(\mathbf{x}|\boldsymbol{\Theta})=\text{GNN }(\mathbf{x})\\
	\frac{d\mathbf{x}}{dt}(\mathbf{x}) &=& \mathbf{J}\left[\frac{\partial H_{\Theta}}{\partial\mathbf{p}}(\mathbf{x}),-\frac{\partial H_{\Theta}}{\partial\mathbf{q}}(\mathbf{x})\right]^T
\end{eqnarray}

This model gives trajectory trough using of one ode solvers.

\section{Gated Recurrent Graph Hamiltonian Neural Network - GRUGHNN}
In previous section we introduced the GHNN, which is base an hamiltonian layer and odesolver. This architecture is not different but imporved with a GRU unit at the begining of the input. this model is equivalent to the GRU stepper but we added a GHNN in it.

\begin{eqnarray}
	[\mathbf{a}_i, \mathbf{h}_{i+1}] &=&  \text{GRU}(\mathbf{x}_i,\mathbf{h}_i)\\
	 H_i&=& \text{GNN}(\mathbf{a}_i)\\
	 \mathbf{v}_i&=&\mathbf{J}\left[\frac{\partial H_{i}}{\partial\mathbf{p}}(\mathbf{x}),-\frac{\partial H_{i}}{\partial\mathbf{q}}(\mathbf{x})\right]^T\\
	 \mathbf{x}_{i+1} &=& \mathbf{x}_i + \mathbf{v}_i \cdot dt
\end{eqnarray}
